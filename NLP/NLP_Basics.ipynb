{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Natural Language Processing (NLP) is concerned with the interactions between computers and human (natural) languages.\n",
        "# It focuses on how computers can process and analyze large amounts of natural language data.\n",
        "# Unlike numerical data, which computers excel at handling, text data is often highly unstructured.\n",
        "# NLP uses various techniques to create structure from raw text data.\n",
        "# Example techniques include:\n",
        "# - Classifying emails as spam or legitimate\n",
        "# - Performing sentiment analysis on reviews\n",
        "# - Understanding and processing text commands"
      ],
      "metadata": {
        "id": "dvT4rLLz8h3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsNQs4352m9k"
      },
      "outputs": [],
      "source": [
        "# SpaCy: Open Source NLP Library\n",
        "# Handles NLP tasks with the most efficient implementation of common algorithms.\n",
        "# Uses a single implementation method (the most efficient currently available).\n",
        "# Better performance in most common NLP tasks.\n",
        "# Users cannot choose the algorithmic implementation (defaults to the most efficient).\n",
        "# Some applications are not included (such as sentiment analysis).\n",
        "\n",
        "# NLTK: Popular Open Source NLP Library\n",
        "# Less efficient implementation.\n",
        "# NLTK is easier to use for certain tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of Keywords:\n",
        "# - `import spacy`: Imports the SpaCy library for NLP tasks.\n",
        "# - `nlp`: A SpaCy model object used to process raw text data (e.g., `nlp = spacy.load('en_core_web_sm')`).\n",
        "# - `spacy.load('en_core_web_sm')`: Loads the English language model for SpaCy.\n",
        "# - `doc`: A processed SpaCy document object that contains tokens and their linguistic annotations (e.g., `doc = nlp(\"Tesla is looking at buying U.S. startup for $6 million\")`).\n",
        "# - `token.text`: Accesses the text of a token in a SpaCy document.\n",
        "# - `token.pos_`: Accesses the part-of-speech tag of a token.\n",
        "# - `token.dep_`: Accesses the syntactic dependency label of a token.\n",
        "# - `token.lemma_`: Accesses the lemma (base form) of a token.\n",
        "# - `doc.sents`: An iterator over sentence spans in the document.\n",
        "# - `entity`: Refers to a named entity in the text.\n",
        "# - `entity.label_`: Accesses the label (type) of a named entity.\n",
        "# - `spacy.explain(entity.label_)`: Provides a human-readable explanation of the entity label.\n",
        "# - `for token in doc`: Iterates over each token in the SpaCy document.\n",
        "# - `for entity in doc.ents`: Iterates over each named entity in the SpaCy document.\n"
      ],
      "metadata": {
        "id": "QhokeE-YKZLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SpaCy\n",
        "!pip install spacy\n",
        "\n",
        "# Download the English language model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "UunOBCei7hw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SpaCy with nlp() processes raw text data and performs various NLP operations such as tokenization, tagging, and parsing.\n",
        "# These operations include part-of-speech recognition and more, resulting in a processed document.\n"
      ],
      "metadata": {
        "id": "0_3u0RpO8EGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the SpaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process raw text data\n",
        "doc = nlp(\"Tesla is looking at buying U.S. startup for $6 million\")\n",
        "\n",
        "# The nlp() function processes raw text and performs NLP tasks like tokenization, tagging, parsing, and part-of-speech recognition, resulting in a processed document.\n"
      ],
      "metadata": {
        "id": "cCKiIXy492cl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_, token.lemma_)  # dep_: syntatic dependency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm1qbWA7_AWz",
        "outputId": "f838a3be-fd21-4fee-d72f-f715ac85493c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla PROPN nsubj Tesla\n",
            "is AUX aux be\n",
            "looking VERB ROOT look\n",
            "at ADP prep at\n",
            "buying VERB pcomp buy\n",
            "U.S. PROPN compound U.S.\n",
            "startup NOUN dobj startup\n",
            "for ADP prep for\n",
            "$ SYM quantmod $\n",
            "6 NUM compound 6\n",
            "million NUM pobj million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uf1J_cb_K61",
        "outputId": "a71a15c6-934b-4533-9a5b-d376088fae3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7b86d5ccef80>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7b86d5ccff40>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7b86d9d04a50>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7b86d5aefe40>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7b86d5aef240>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7b86d9d07d10>)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlJPNCaGAHj8",
        "outputId": "b6772483-afd9-4ae3-87f4-82eb71f6d146"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(\"Tesla isn't     looking for startups anymore.\")"
      ],
      "metadata": {
        "id": "WfsXo7mVAMzN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc2:\n",
        "  print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL6Bm2CcAY9C",
        "outputId": "76083488-bf96-41b9-e17d-85f798a6ee7a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla PROPN nsubj\n",
            "is AUX aux\n",
            "n't PART neg\n",
            "     SPACE dep\n",
            "looking VERB ROOT\n",
            "for ADP prep\n",
            "startups NOUN pobj\n",
            "anymore ADV advmod\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing to grab token individually\n",
        "doc2[0].pos_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kG2VnvhpA09U",
        "outputId": "41d8e2d6-c318-4721-a8cc-af4a0dc65c18"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PROPN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(\"This is the first setnence. This is another sentence. This is the last sentence\")"
      ],
      "metadata": {
        "id": "ITUouUPwDAZ3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the sentences in the doc object and print each sentence\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuvLdrpMDInm",
        "outputId": "f399f440-01c9-4a14-ae37-43b0e3db64ec"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla is looking at buying U.S. startup for $6 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3[7].is_sent_start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9Dz4Y5IDWxz",
        "outputId": "be208155-1fcd-45f9-a2a1-a5b78bfb445b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "NpkYzPryDrra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization is the process of breaking up the original text into component pieces (tokens).\n",
        "# Tokens are the basic building blocks of the doc object.\n",
        "# Everything that helps understand the meaning of the text is derived from the tokens and their relationships.\n",
        "# Prefix examples: $, (, \"\n",
        "# Suffix examples: km, ), ,, ., !\n",
        "# Infix examples: -, --, /, ...\n",
        "# Exceptions: Handles cases like \"let's\" or \"U.S.\" to split or prevent splitting into multiple tokens."
      ],
      "metadata": {
        "id": "9OpAt_kfDrYy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mystring = '\"We\\'re moving to L.A.!\"'"
      ],
      "metadata": {
        "id": "H6zjvz_8FKB8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mystring"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "j9ngPTV7FXFL",
        "outputId": "400c9c9e-d0e5-42aa-c47b-79004d077a3a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"We\\'re moving to L.A.!\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mystring)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1ugIsRFbCt",
        "outputId": "2b5c1aa4-e580-477f-c5ea-6b4b18a4aa08"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"We're moving to L.A.!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(mystring)"
      ],
      "metadata": {
        "id": "A83UXWcEFecR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc4:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69uSe4dsFhXh",
        "outputId": "3d7ced0a-5362-4e39-a6e2-1bdf4e43f32f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "We\n",
            "'re\n",
            "moving\n",
            "to\n",
            "L.A.\n",
            "!\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc5 = nlp(\"We're here to help! send snail-mail or email nawarturk11@gmail.com or visit www.github.com/nawarturk\")"
      ],
      "metadata": {
        "id": "4fusBkuuFhRF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc5:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH9KvRlPF-PT",
        "outputId": "848d3e51-cbad-4621-ca57-1fb013a7e3fd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We\n",
            "'re\n",
            "here\n",
            "to\n",
            "help\n",
            "!\n",
            "send\n",
            "snail\n",
            "-\n",
            "mail\n",
            "or\n",
            "email\n",
            "nawarturk11@gmail.com\n",
            "or\n",
            "visit\n",
            "www.github.com/nawarturk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc6 = nlp(\"let's visit St. Louis in the U.S. next year.\")"
      ],
      "metadata": {
        "id": "99dSwU7aGX8X"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc6:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jTxnTqZGrwM",
        "outputId": "a1d61e62-e09b-42bb-f1f4-4cca619a375d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "let\n",
            "'s\n",
            "visit\n",
            "St.\n",
            "Louis\n",
            "in\n",
            "the\n",
            "U.S.\n",
            "next\n",
            "year\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr1TZbp-G7Cj",
        "outputId": "d3754cda-c823-4f19-8013-181060984ccf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doc5[5] = 'test'  # You cannot reassign a token in a SpaCy doc object."
      ],
      "metadata": {
        "id": "xvTCOXgMHYiy"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc7 = nlp('Apple to build a Hong Kong factory for $6 million')"
      ],
      "metadata": {
        "id": "-6KYEuNzHmCB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc7:\n",
        "  print(token.text, end='|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgXljCnDHtZS",
        "outputId": "6bfdf1d7-77ac-4bfb-f134-81c72f736ece"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple|to|build|a|Hong|Kong|factory|for|$|6|million|"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the named entities in the doc7 object\n",
        "for entity in doc7.ents:\n",
        "    print(entity)  # Print the entity\n",
        "    print(entity.label_)  # Print the entity's label\n",
        "    print(str(spacy.explain(entity.label_)))  # Print an explanation of the entity's label\n",
        "    print('\\n')\n",
        "\n",
        "# This recognizes and prints named entities from the text.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA6-q-RYH2q6",
        "outputId": "df09c277-dc9a-4c0d-ee8f-7f489a731760"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple\n",
            "ORG\n",
            "Companies, agencies, institutions, etc.\n",
            "\n",
            "\n",
            "Hong Kong\n",
            "GPE\n",
            "Countries, cities, states\n",
            "\n",
            "\n",
            "$6 million\n",
            "MONEY\n",
            "Monetary values, including unit\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc8 = nlp('Autonomous cars shift insurance liability toward manufacturers.')"
      ],
      "metadata": {
        "id": "0k2BWZD4IwnO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in doc8.noun_chunks:\n",
        "  print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHbrJNA1I4_8",
        "outputId": "3f8bed93-2187-458f-fc4b-7821fbbf427c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous cars\n",
            "insurance liability\n",
            "manufacturers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the sentence \"Tesla is looking at buying Hong Kong startup for $6 million\":\n",
        "# - Tokens are individual words or punctuation marks, e.g., \"Tesla\", \"is\", \"looking\", \"at\", \"buying\", \"Hong\", \"Kong\", \"startup\", \"for\", \"$\", \"6\", \"million\".\n",
        "# - Entities are specific named objects, e.g., \"Tesla\" (an organization) and \"Hong Kong\" (a geopolitical entity).\n",
        "# - Chunks are meaningful phrases centered around nouns, e.g., \"Tesla\", \"Hong Kong startup\", and \"$6 million\".\n"
      ],
      "metadata": {
        "id": "e7AefiUrKdtN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}